# .github/workflows/check-data-updates.yml
# 
# This workflow monitors Michigan lead data sources for updates and creates
# GitHub Issues to alert when changes are detected.
#
# Data sources monitored:
# 1. Michigan 90th Percentile API (Socrata)
# 2. EGLE DSMI Inventories page
# 3. EGLE Lead & Copper Rule page
#
# Runs daily at 9 AM Eastern (14:00 UTC)

name: Check for Data Updates

on:
  schedule:
    # Run daily at 9 AM Eastern (14:00 UTC, or 13:00 during DST)
    - cron: '0 14 * * *'
  workflow_dispatch:  # Allow manual trigger

jobs:
  check-updates:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4

      - name: Check for data updates
        id: check
        run: |
          python << 'EOF'
          import requests
          import hashlib
          import json
          import os
          from datetime import datetime
          from bs4 import BeautifulSoup

          # File to store previous state
          STATE_FILE = '.data-check-state.json'

          def load_state():
              """Load previous check state from file."""
              try:
                  with open(STATE_FILE, 'r') as f:
                      return json.load(f)
              except FileNotFoundError:
                  return {}

          def save_state(state):
              """Save current state to file."""
              with open(STATE_FILE, 'w') as f:
                  json.dump(state, f, indent=2)

          def get_content_hash(content):
              """Generate MD5 hash of content."""
              return hashlib.md5(content.encode('utf-8')).hexdigest()

          def check_socrata_api():
              """
              Check Michigan 90th Percentile API for changes.
              Returns dict with status info.
              """
              url = "https://data.michigan.gov/resource/39ya-9txc.json"
              try:
                  # Get record count
                  count_url = f"{url}?$select=count(*)"
                  count_resp = requests.get(count_url, timeout=30)
                  count_data = count_resp.json()
                  record_count = int(count_data[0]['count']) if count_data else 0
                  
                  # Get sample of recent records for hash comparison
                  # Sort by any date field if available, otherwise just get first 100
                  sample_url = f"{url}?$limit=100&$order=:id"
                  sample_resp = requests.get(sample_url, timeout=30)
                  sample_data = sample_resp.json()
                  content_hash = get_content_hash(json.dumps(sample_data, sort_keys=True))
                  
                  return {
                      'success': True,
                      'record_count': record_count,
                      'content_hash': content_hash,
                      'url': 'https://data.michigan.gov/dataset/Public-Water-Supply-90th-Percentiles/39ya-9txc'
                  }
              except Exception as e:
                  return {'success': False, 'error': str(e)}

          def check_webpage(url, name):
              """
              Check a webpage for content changes.
              Returns dict with status info.
              """
              try:
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (compatible; DataMonitor/1.0; +https://planetdetroit.org)'
                  }
                  resp = requests.get(url, headers=headers, timeout=30)
                  resp.raise_for_status()
                  
                  # Parse HTML and extract main content (ignore dynamic elements)
                  soup = BeautifulSoup(resp.text, 'html.parser')
                  
                  # Remove script, style, and other dynamic elements
                  for element in soup(['script', 'style', 'nav', 'footer', 'header']):
                      element.decompose()
                  
                  # Get text content
                  text_content = soup.get_text(separator=' ', strip=True)
                  content_hash = get_content_hash(text_content)
                  
                  # Look for links to data files (CSV, Excel, etc.)
                  data_links = []
                  for link in soup.find_all('a', href=True):
                      href = link['href'].lower()
                      if any(ext in href for ext in ['.csv', '.xlsx', '.xls', '.zip', '.pdf']):
                          data_links.append(link['href'])
                  
                  return {
                      'success': True,
                      'content_hash': content_hash,
                      'data_links_count': len(data_links),
                      'url': url
                  }
              except Exception as e:
                  return {'success': False, 'error': str(e), 'url': url}

          def main():
              print("=" * 60)
              print(f"Data Update Check - {datetime.now().isoformat()}")
              print("=" * 60)
              
              # Load previous state
              prev_state = load_state()
              current_state = {}
              changes = []
              
              # 1. Check Socrata API (90th Percentile data)
              print("\n[1/3] Checking Michigan 90th Percentile API...")
              api_result = check_socrata_api()
              current_state['socrata_api'] = api_result
              
              if api_result['success']:
                  print(f"  âœ“ Record count: {api_result['record_count']}")
                  print(f"  âœ“ Content hash: {api_result['content_hash'][:12]}...")
                  
                  prev_api = prev_state.get('socrata_api', {})
                  if prev_api.get('record_count') and prev_api['record_count'] != api_result['record_count']:
                      changes.append({
                          'source': '90th Percentile API (Socrata)',
                          'type': 'Record count changed',
                          'details': f"Previous: {prev_api['record_count']}, Current: {api_result['record_count']}",
                          'url': api_result['url']
                      })
                  elif prev_api.get('content_hash') and prev_api['content_hash'] != api_result['content_hash']:
                      changes.append({
                          'source': '90th Percentile API (Socrata)',
                          'type': 'Data content changed',
                          'details': 'Record content has been modified',
                          'url': api_result['url']
                      })
              else:
                  print(f"  âœ— Error: {api_result.get('error')}")
              
              # 2. Check DSMI Inventories page
              print("\n[2/3] Checking EGLE DSMI Inventories page...")
              dsmi_url = "https://www.michigan.gov/egle/about/organization/drinking-water-and-environmental-health/community-water-supply/lead-and-copper-rule/dsmi-inventories"
              dsmi_result = check_webpage(dsmi_url, "DSMI Inventories")
              current_state['dsmi_page'] = dsmi_result
              
              if dsmi_result['success']:
                  print(f"  âœ“ Content hash: {dsmi_result['content_hash'][:12]}...")
                  print(f"  âœ“ Data file links found: {dsmi_result['data_links_count']}")
                  
                  prev_dsmi = prev_state.get('dsmi_page', {})
                  if prev_dsmi.get('content_hash') and prev_dsmi['content_hash'] != dsmi_result['content_hash']:
                      changes.append({
                          'source': 'EGLE DSMI Inventories Page',
                          'type': 'Page content changed',
                          'details': 'The DSMI inventories page has been updated - new data may be available',
                          'url': dsmi_url
                      })
              else:
                  print(f"  âœ— Error: {dsmi_result.get('error')}")
              
              # 3. Check Lead & Copper Rule page
              print("\n[3/3] Checking EGLE Lead & Copper Rule page...")
              lcr_url = "https://www.michigan.gov/egle/about/organization/drinking-water-and-environmental-health/community-water-supply/lead-and-copper-rule"
              lcr_result = check_webpage(lcr_url, "Lead & Copper Rule")
              current_state['lcr_page'] = lcr_result
              
              if lcr_result['success']:
                  print(f"  âœ“ Content hash: {lcr_result['content_hash'][:12]}...")
                  print(f"  âœ“ Data file links found: {lcr_result['data_links_count']}")
                  
                  prev_lcr = prev_state.get('lcr_page', {})
                  if prev_lcr.get('content_hash') and prev_lcr['content_hash'] != lcr_result['content_hash']:
                      changes.append({
                          'source': 'EGLE Lead & Copper Rule Page',
                          'type': 'Page content changed',
                          'details': 'The Lead & Copper Rule page has been updated',
                          'url': lcr_url
                      })
              else:
                  print(f"  âœ— Error: {lcr_result.get('error')}")
              
              # Save current state
              current_state['last_check'] = datetime.now().isoformat()
              save_state(current_state)
              
              # Report results
              print("\n" + "=" * 60)
              if changes:
                  print(f"ðŸš¨ CHANGES DETECTED: {len(changes)} update(s) found!")
                  for i, change in enumerate(changes, 1):
                      print(f"\n  [{i}] {change['source']}")
                      print(f"      Type: {change['type']}")
                      print(f"      Details: {change['details']}")
                      print(f"      URL: {change['url']}")
                  
                  # Set output for GitHub Actions
                  changes_json = json.dumps(changes)
                  with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
                      f.write(f"changes_detected=true\n")
                      f.write(f"changes_count={len(changes)}\n")
                      # Escape for GitHub Actions
                      escaped_json = changes_json.replace('%', '%25').replace('\n', '%0A').replace('\r', '%0D')
                      f.write(f"changes_json={escaped_json}\n")
              else:
                  print("âœ“ No changes detected. All data sources unchanged.")
                  with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
                      f.write("changes_detected=false\n")
                      f.write("changes_count=0\n")
              
              print("=" * 60)

          if __name__ == '__main__':
              main()
          EOF

      - name: Commit state file
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add .data-check-state.json || true
          git diff --staged --quiet || git commit -m "Update data check state [skip ci]"
          git push || true

      - name: Create issue if changes detected
        if: steps.check.outputs.changes_detected == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const changesCount = '${{ steps.check.outputs.changes_count }}';
            const changesJson = decodeURIComponent('${{ steps.check.outputs.changes_json }}'.replace(/%0A/g, '\n').replace(/%0D/g, '\r').replace(/%25/g, '%'));
            
            let changes;
            try {
              changes = JSON.parse(changesJson);
            } catch (e) {
              changes = [{ source: 'Unknown', type: 'Parse error', details: changesJson }];
            }
            
            const date = new Date().toISOString().split('T')[0];
            
            let body = `## ðŸš¨ Data Source Update Detected\n\n`;
            body += `**Date:** ${date}\n`;
            body += `**Changes Found:** ${changesCount}\n\n`;
            body += `---\n\n`;
            
            for (const change of changes) {
              body += `### ${change.source}\n\n`;
              body += `- **Type:** ${change.type}\n`;
              body += `- **Details:** ${change.details}\n`;
              body += `- **URL:** ${change.url}\n\n`;
            }
            
            body += `---\n\n`;
            body += `### Recommended Actions\n\n`;
            body += `1. Review the changed data source(s)\n`;
            body += `2. Download updated data if applicable\n`;
            body += `3. Run the conversion script: \`node scripts/convertCsv.js\`\n`;
            body += `4. Test locally: \`npm start\`\n`;
            body += `5. Deploy: \`npm run deploy\`\n\n`;
            body += `*This issue was automatically created by the data monitoring workflow.*`;
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `ðŸ“Š Data Update Alert: ${changesCount} source(s) changed - ${date}`,
              body: body,
              labels: ['data-update', 'automated']
            });
